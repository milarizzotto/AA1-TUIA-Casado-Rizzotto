{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características y etiquetas\n",
    "X_train = pd.read_csv('./files/X_train.csv')\n",
    "y_train = np.where(pd.read_csv('./files/y_train.csv') == 'Yes', 1, 0)  # Convertir etiquetas a 1/0\n",
    "y_train = pd.DataFrame(y_train, columns=['RainTomorrow_Yes'])\n",
    "# Separar características y etiquetas\n",
    "X_test = pd.read_csv('./files/X_test.csv')\n",
    "y_test = np.where(pd.read_csv('./files/y_test.csv') == 'Yes', 1, 0)  # Convertir etiquetas a 1/0\n",
    "y_test = pd.DataFrame(y_test, columns=['RainTomorrow_Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DateMonthExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_column=\"Date\", new_column=\"Month\"):\n",
    "        self.date_column = date_column\n",
    "        self.new_column = new_column\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No necesita ajuste\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()  # Crear una copia para no modificar los datos originales\n",
    "        X[self.date_column] = pd.to_datetime(X[self.date_column])  # Convertir a datetime\n",
    "        X[self.new_column] = X[self.date_column].dt.month  # Extraer el mes\n",
    "        X.drop(columns=[self.date_column], inplace=True)  # Eliminar la columna original\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ProbImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns  # Columnas a imputar\n",
    "        self.value_probs = {}  # Diccionario para almacenar valores únicos y probabilidades\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            values = X[col].dropna().value_counts(normalize=True)\n",
    "            self.value_probs[col] = (values.index.values, values.values)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()  # Crear una copia para no modificar el original\n",
    "        for col in self.columns:\n",
    "            if col in self.value_probs:\n",
    "                values, probs = self.value_probs[col]\n",
    "                X[col] = X[col].apply(\n",
    "                    lambda x: np.random.choice(values, p=probs) if pd.isnull(x) else x\n",
    "                )\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Imputación para columnas numéricas con la mediana\n",
    "def apply_median_imputation(X, columns):\n",
    "    imputer_median = SimpleImputer(strategy=\"median\")\n",
    "    X[columns] = imputer_median.fit_transform(X[columns])\n",
    "    return X\n",
    "\n",
    "# Imputación usando KNN para columnas numéricas\n",
    "def apply_knn_imputation(X, columns):\n",
    "    knn_imputer = KNNImputer()\n",
    "    X[columns] = knn_imputer.fit_transform(X[columns])\n",
    "    return X\n",
    "\n",
    "# Escalado de características numéricas\n",
    "def apply_scaling(X):\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DummiesEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_cols):\n",
    "        self.categorical_cols = categorical_cols\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No necesita ajuste\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)  # Convertir a DataFrame si es un ndarray\n",
    "        X = X.copy()  # Crear una copia para no modificar los datos originales\n",
    "        \n",
    "        # Verificar si las columnas están presentes en X\n",
    "        missing_cols = [col for col in self.categorical_cols if col not in X.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Las siguientes columnas no están en el DataFrame: {missing_cols}\")\n",
    "        \n",
    "        # Convertir las columnas categóricas a tipo string\n",
    "        X.loc[:, self.categorical_cols] = X.loc[:, self.categorical_cols].astype(str)\n",
    "        \n",
    "        # Aplicar pd.get_dummies\n",
    "        X = pd.get_dummies(X, columns=self.categorical_cols, drop_first=True)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_nn_model(input_shape, learning_rate=0.001025, dropout_rate=0.2454, l2_lambda=0.002, units_per_layer=116):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Capa de entrada con el tamaño correcto\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    # Añadir capas ocultas con parámetros óptimos\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "            units_per_layer, \n",
    "            activation='relu', \n",
    "            kernel_regularizer=tf.keras.regularizers.L2(l2_lambda)\n",
    "        ))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Capa de salida\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Configurar optimizador\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def train_and_evaluate_nn(X_train, y_train, X_test, y_test, categorical_cols, numerical_median_cols, numerical_knn_cols):\n",
    "    # Transformaciones: imputación, codificación y escalado\n",
    "    X_train = apply_median_imputation(X_train, numerical_median_cols)\n",
    "    X_test = apply_median_imputation(X_test, numerical_median_cols)\n",
    "    X_train = apply_knn_imputation(X_train, numerical_knn_cols)\n",
    "    X_test = apply_knn_imputation(X_test, numerical_knn_cols)\n",
    "\n",
    "    # Crear transformadores adicionales (imputación probabilística y extracción de fecha)\n",
    "    prob_imputer = ProbImputer(columns=categorical_cols)\n",
    "    date_transformer = DateMonthExtractor(date_column=\"Date\", new_column=\"Month\")\n",
    "    \n",
    "    # Transformar los datos de entrenamiento y prueba\n",
    "    X_train = prob_imputer.fit_transform(X_train)\n",
    "    X_test = prob_imputer.transform(X_test)\n",
    "    X_train = date_transformer.fit_transform(X_train)\n",
    "    X_test = date_transformer.transform(X_test)\n",
    "    \n",
    "    # Codificar variables categóricas\n",
    "    encoder = DummiesEncoder(categorical_cols=categorical_cols)\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    joblib.dump(X_train.columns.tolist(), './docker/train_columns.pkl')\n",
    "    # Escalar las características\n",
    "    X_train = apply_scaling(X_train)\n",
    "    X_test = apply_scaling(X_test)\n",
    "    \n",
    "    # Guardar las columnas después del preprocesamiento durante el entrenamiento\n",
    "\n",
    "    # Crear el modelo con el input_shape correcto\n",
    "    input_shape = (X_train.shape[1],)  # El número de características\n",
    "    \n",
    "    model = create_nn_model(input_shape=input_shape)\n",
    "\n",
    "    # Entrenamiento del modelo directamente con TensorFlow\n",
    "    model.fit(X_train, y_train, epochs=81, batch_size=64, verbose=0)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5).astype(int)  # Convertir a clases binarias\n",
    "    \n",
    "    # Evaluación del modelo\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "    print(f\"F1-Score: {f1}\")\n",
    "    \n",
    "    return model, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\TP_AA1\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "F1-Score: 0.83918068571204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./docker/nn_model.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Variables de entrada (X_train, y_train, X_test, y_test ya deberían estar definidos previamente)\n",
    "categorical_cols = [\"WindDir3pm\", \"WindDir9am\", \"WindGustDir\", \"RainToday\", \"Location\"]\n",
    "numerical_median_cols = [\"Pressure3pm\", \"Pressure9am\", \"Temp3pm\", \"Temp9am\", \"MinTemp\", \"MaxTemp\"]\n",
    "numerical_knn_cols = [\"Evaporation\", \"Rainfall\", \"Humidity3pm\", \"WindSpeed3pm\", \"WindSpeed9am\", \"Cloud3pm\", \"Cloud9am\", \"Humidity9am\", \"Sunshine\", \"WindGustSpeed\"]\n",
    "\n",
    "# Entrenar y evaluar el modelo\n",
    "nn_model, f1 = train_and_evaluate_nn(X_train, y_train, X_test, y_test, categorical_cols, numerical_median_cols, numerical_knn_cols)\n",
    "\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "joblib.dump(nn_model, './docker/nn_model.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP_AA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
